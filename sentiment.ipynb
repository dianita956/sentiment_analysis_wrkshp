{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHfLISKRVQWd"
      },
      "source": [
        "Sentiment Analysis\n",
        "==================\n",
        "presented by: [Diane López] (https://github.com/dianita956)\n",
        "April 2, 2025\n",
        "CEDISH - University of Texas at San Antonio Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9toYwOzlVQWn"
      },
      "source": [
        "### Sentiment Analysis for Academic Research\n",
        "Sentiment analysis is also widely used in academic research to analyze large volumes of text data,\n",
        "such as student feedback, academic publications, and social media interactions related to educational topics.\n",
        "It helps in understanding trends, opinions, and the overall sentiment in the academic community."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0e7zITNVQWn"
      },
      "source": [
        "### Workshop Agenda\n",
        "1. Welcome & Check-In\n",
        "2. Setting up your environment\n",
        "3. Hands-On Practice\n",
        "4. Q&A\n",
        "5. Wrap-up\n",
        "    - resources list\n",
        "    - feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1yRCmWjVQWn"
      },
      "source": [
        "### Introduction to Sentiment Analysis\n",
        "\n",
        "#### Techniques\n",
        "\n",
        "- **Lexicon-based: Uses predefined word lists associated with sentiments.**\n",
        "\n",
        "- Machine Learning: Models trained on labeled data (e.g., Naive Bayes, SVM).\n",
        "\n",
        "- Deep Learning: Uses advanced neural networks like RNNs and CNNs for complex sentiment patterns.\n",
        "\n",
        "- Hybrid: Combines lexicon-based and ML approaches for better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iJfmWhhVQWo"
      },
      "source": [
        "## Sentiment Analysis in Python\n",
        "\n",
        "Sentiment Analysis in Python uses NLP techniques to identify the emotional tone in text.\n",
        "\n",
        "It classifies sentiment as positive, negative, or neutral, aiding applications like social media monitoring and customer feedback analysis.\n",
        "\n",
        "Python provides various powerful libraries for this purpose.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSjVH7uEVQWo"
      },
      "source": [
        "### Creating your own sentiment dictionary and analyzer\n",
        "\n",
        "While there are many great sentiment dictionaries available, they may not always be the right fit. In such cases, building your own can be a great option!\n",
        "\n",
        "| Situation | Prebuilt dictionary | Create Custom Directionary |\n",
        "| :-------- | :-----------------: | :------------------------: |\n",
        "| General sentiment analysis |  ✅  |                         ❌ |\n",
        "| Domain Specific text | ❌ | ✅ |\n",
        "| Large-scale quick turnaround | ✅ | ❌ |\n",
        "| High accuracy (niche area) | ❌ | ✅ |\n",
        "| Specific phrasing | ❌ | ✅ |\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib\n",
        "!pip install vaderSentiment\n",
        "!pip install -U nltk\n",
        "!pip install -U deep-translator"
      ],
      "metadata": {
        "id": "cIIdDBoNgVKO",
        "outputId": "be69fc8c-3a95-4d34-858e-9ff02d7204b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from vaderSentiment) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->vaderSentiment) (2025.1.31)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: deep-translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.13.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_8shbnOlVQWo",
        "outputId": "b5612390-4cab-4dc6-a1f1-6787ef3c8ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'SylviaGonzalez_CasaLatina.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-05a16d5f9566>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#example text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'SylviaGonzalez_CasaLatina.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SylviaGonzalez_CasaLatina.txt'"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "#building a simple Spanish Lexicon (dictionary)\n",
        "#postive words score greater > than 0\n",
        "#negative words score less < than 0\n",
        "#neutral words score equal = to 0\n",
        "\n",
        "\n",
        "positive_words = [\n",
        "    'bueno', 'mágico', 'paz', 'amistad', 'deseo',\n",
        "    'feliz', 'alegría', 'amor', 'hermoso', 'éxito',\n",
        "    'satisfacción', 'genial', 'increíble', 'niña', 'pueblo',\n",
        "    'bosque', 'criaturas', 'hada', 'regreso', 'naturaleza'\n",
        "]\n",
        "\n",
        "negative_words = [\n",
        "    'triste', 'odio', 'miedo', 'fracaso', 'desastre',\n",
        "    'decepción', 'perderse', 'desaparecer', 'nerviosa'\n",
        "    ]\n",
        "\n",
        "neutral_words = [\n",
        "    'normal', 'regular', 'común',\n",
        "    'promedio', 'típico', 'estándar'\n",
        "    'niña', 'pueblo', 'bosque', 'criaturas',\n",
        "    'hada', 'regreso', 'naturaleza', 'foto'\n",
        "    ]\n",
        "\n",
        "#creating a dictionary with the words and their scores\n",
        "lexicon = {}\n",
        "for word in positive_words: # the colon indicates that the next block of code should be repeated a number of times. iterate thru the list\n",
        "    lexicon[word] = 1\n",
        "for word in negative_words:\n",
        "    lexicon[word] = -1\n",
        "for word in neutral_words:\n",
        "    lexicon[word] = 0\n",
        "\n",
        "#checking lexicon dictionary and word score\n",
        "#print(lexicon)\n",
        "\n",
        "#creating a function to calculate the sentiment score of a text\n",
        "def calculate_sentiment(text):\n",
        "    #tokenizing the text using regex and lowercasing and find words\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    #store works and scores in a dictionary\n",
        "    word_scores = {}\n",
        "    total_score = 0\n",
        "\n",
        "    for word in set(tokens): # set() is a Python built-in function used to create a set object. A set is unorder and part of data types: list, tuples,, and dictionary. it help to remoive dups.\n",
        "        if word in lexicon:\n",
        "            word_scores[word] = word_scores.get(word, 0) + lexicon[word] #creating a list to store words and its score\n",
        "            total_score = total_score + lexicon[word]\n",
        "            #print(total_score)\n",
        "\n",
        "    for word, score in word_scores.items():\n",
        "        print(f\"{word}:{score}\")\n",
        "\n",
        "    #calculate sentiment score: 1 = positive, -1 = negative, 0 = neutral\n",
        "    return total_score, word_scores\n",
        "\n",
        "#example text\n",
        "with open('SylviaGonzalez_CasaLatina.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "#calcuating sentiment score\n",
        "sentiment_score, word_scores = calculate_sentiment(text)\n",
        "print(f\"Sentiment Score: {sentiment_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzV8PgqzVQWp"
      },
      "source": [
        "#### Bar Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuAZ66oAVQWp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Counting positive, negative, and neutral words\n",
        "positive_count = sum(1 for word, score in lexicon.items() if word in word_scores and score > 0)\n",
        "negative_count = sum(1 for word, score in lexicon.items() if word in word_scores and score < 0)\n",
        "neutral_count = sum(1 for word, score in lexicon.items() if word in word_scores and score == 0)\n",
        "\n",
        "# Creating a bar chart\n",
        "categories = ['Positive', 'Negative', 'Neutral']\n",
        "counts = [positive_count, negative_count, neutral_count]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(categories, counts, color=['green', 'red', 'black'])\n",
        "plt.xlabel('Sentiment Category')\n",
        "plt.ylabel('Word Count')\n",
        "plt.title('Sentiment Distribution in Text')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUTiNwNNVQWp"
      },
      "outputs": [],
      "source": [
        "#example text\n",
        "with open('Sofia.txt', 'r', encoding='utf-8') as f:\n",
        "    text2 = f.read()\n",
        "\n",
        "#calcuating sentiment score\n",
        "sentiment_score, word_scores = calculate_sentiment(text2)\n",
        "print(f\"Sentiment Score: {sentiment_score}\")\n",
        "\n",
        " # Counting positive, negative, and neutral words\n",
        "positive_count = sum(1 for word, score in lexicon.items() if word in word_scores and score > 0)\n",
        "negative_count = sum(1 for word, score in lexicon.items() if word in word_scores and score < 0)\n",
        "neutral_count = sum(1 for word, score in lexicon.items() if word in word_scores and score == 0)\n",
        "\n",
        "# Creating a bar chart\n",
        "categories = ['Positive', 'Negative', 'Neutral']\n",
        "counts = [positive_count, negative_count, neutral_count]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(categories, counts, color=['green', 'red', 'black'])\n",
        "plt.xlabel('Sentiment Category')\n",
        "plt.ylabel('Word Count')\n",
        "plt.title('Sentiment Distribution in Text')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buH3486rVQWq"
      },
      "source": [
        "## Vader Sentiment and Deep Translator\n",
        "\n",
        "- Vader Sentiment https://github.com/cjhutto/vaderSentiment\n",
        "- Deep Translator https://github.com/nidhaloff/deep-translator\n",
        "\n",
        "This code performs sentiment analysis on a Spanish Language text file by following a steps to clean, split, translate, and analyze the text.\n",
        "\n",
        "It starts by using SentimentIntensityAnalyzer from VADER to run sentiment analysis on the translated text.\n",
        "\n",
        "1.\tremove_timestamps(): This function removes any timestamps in the format [hh:mm:ss.xxx] from the input text using a regular expression pattern. It ensures that the text is clean and does not contain unnecessary time data before performing any analysis.\n",
        "2.\tsplit_text(): This function breaks the cleaned text into smaller chunks, each of a maximum length of 5000 characters, without splitting sentences inappropriately. The function splits the text at sentence boundaries and organizes it into chunks, ensuring that each chunk fits within the specified character limit.\n",
        "3.\tTranslation: The GoogleTranslator from the Deep Translator library is used to translate the Spanish text chunks into English. Each chunk is translated separately, and the translated text is concatenated back together to form a complete translated version.\n",
        "4.\tSentiment Analysis: After translation, the VADER SentimentIntensityAnalyzer is applied to the translated text to assess its overall sentiment. The function polarity_scores() returns a dictionary with sentiment scores, including the positive, negative, neutral, and compound scores, giving a comprehensive view of the text’s emotional tone.\n",
        "\n",
        "The final output is a sentiment analysis score for the translated text, which helps determine whether the overall sentiment of the content is positive, negative, or neutral. This process enables sentiment analysis of multilingual content, in this case, Spanish to English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hk5O2l5VQWq"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "from deep_translator import GoogleTranslator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0_SNXw9VQWq"
      },
      "outputs": [],
      "source": [
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def remove_timestamps(text):\n",
        "    # Regular expression pattern to match timestamps [hh:mm:ss.xxx]\n",
        "    pattern = r\"\\[\\d{2}:\\d{2}:\\d{2}\\.\\d{3}\\]\\s*-\\s*\"\n",
        "    # Remove timestamps from the text\n",
        "    cleaned_text = re.sub(pattern, \"\", text)\n",
        "    return cleaned_text\n",
        "\n",
        "clean_text = remove_timestamps(text) # running fuction remove_timestamp\n",
        "#print(clean_text) # Print the cleaned text without timestamps\n",
        "\n",
        "def split_text(text, max_length=5000):\n",
        "    #Splits text into chunks of max_length characters without breaking sentences.\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)  # Split at sentence boundaries\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if len(current_chunk) + len(sentence) <= max_length:\n",
        "            current_chunk += sentence + \" \"\n",
        "        else:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = sentence + \" \"\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks\n",
        "\n",
        "text_chunks = split_text(clean_text) # running function split_text\n",
        "#full_text = \" \".join(text_chunks)\n",
        "\n",
        "translator = GoogleTranslator(source='es', target='en')\n",
        "\n",
        "translated_chunks = [translator.translate(chunk) for chunk in text_chunks]\n",
        "translated_text = \" \".join(translated_chunks)  # Translate Spanish to English\n",
        "\n",
        "with open('SylviaGonzalez_CasaLatina.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "scores = analyzer.polarity_scores(translated_text)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixdcqhcDVQWq"
      },
      "source": [
        "## NLTK\n",
        "\n",
        "- NLTK https://www.nltk.org/\n",
        "\n",
        "\n",
        "This section of code performs sentiment analysis on three distinct text files: Story 1, The Final Algorithm; Story 2, The Forgotten Letter; and Story 3, The Lucky Coin. It utilizes the VADER (Valence Aware Dictionary and Sentiment Reasoner) tool, which is part of the NLTK (Natural Language Toolkit) library. The necessary NLTK resources, such as tokenizers and the VADER lexicon, are downloaded to facilitate sentiment analysis. The SentimentIntensityAnalyzer is initialized to assess the sentiment of the text. Each text file's content is read into memory, and the polarity_scores() method is applied to evaluate the sentiment, producing a dictionary with positive, negative, neutral, and compound sentiment scores. The sentiment scores for each story are printed, providing insights into the emotional tone of the texts. This process is repeated for all three stories, offering a comprehensive sentiment assessment for each text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l9_VJdHVQWq"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey410TW6VQWq"
      },
      "source": [
        "The nltk.download('punkt'), nltk.download('punkt_tab'), and nltk.download('vader_lexicon') commands are used to download essential resources from the Natural Language Toolkit (NLTK) library for text processing and sentiment analysis. The first command, nltk.download('punkt'), downloads the Punkt tokenizer, a pre-trained model designed to split text into sentences and words, which is crucial for text preprocessing tasks like sentence segmentation and tokenization. The second command, nltk.download('punkt_tab'), appears to be an error or incorrect reference, as there is no valid resource by that name in NLTK. The third command, nltk.download('vader_lexicon'), downloads the VADER sentiment lexicon, which is specifically designed for sentiment analysis. This lexicon contains a list of words with associated sentiment scores, helping to determine the overall sentiment of a text (positive, negative, or neutral). Together, these downloads enable the use of NLTK's text tokenization and sentiment analysis tools, facilitating more efficient and accurate text processing for natural language tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuLs0cWNVQWq"
      },
      "outputs": [],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKpPplx5VQWq"
      },
      "source": [
        "The line sia = SentimentIntensityAnalyzer() creates an instance of the SentimentIntensityAnalyzer class from the VADER (Valence Aware Dictionary and sEntiment Reasoner) module in NLTK. This class is specifically designed for sentiment analysis, providing a method to assess the sentiment of a given text. By creating this instance, you can use the polarity_scores() method to analyze text and obtain a set of sentiment scores, including positive, negative, neutral, and compound scores, helping to determine the overall emotional tone of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-IrYDIfVQWq"
      },
      "outputs": [],
      "source": [
        "sia = SentimentIntensityAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhBAkDSNVQWq"
      },
      "source": [
        "This modified code processes three separate text files: story1_the_final_algorithm.txt, story2_the_lucky_coin.txt, and story3_the_secret_quest.txt. For each story, the content is read into a variable, and the SentimentIntensityAnalyzer (sia) is used to calculate the sentiment scores for each text. The polarity_scores() method returns a dictionary with the sentiment analysis results, including positive, negative, neutral, and compound scores. Finally, the sentiment scores for each story are printed, allowing you to compare the overall emotional tone of each one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5pB9SDiVQWq"
      },
      "outputs": [],
      "source": [
        "with open('story1_the_final_algorithm.txt') as f:\n",
        "    text_s1 = f.read()\n",
        "scores = sia.polarity_scores(text_s1)\n",
        "print(f\"Sentiment Scores: {scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LATVqzZhVQWq"
      },
      "outputs": [],
      "source": [
        "with open('story2_the_forgotten_letter.txt') as f:\n",
        "    text_s2 = f.read()\n",
        "scores = sia.polarity_scores(text_s2)\n",
        "print(f\"Sentiment Scores: {scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpq6lzklVQWr"
      },
      "outputs": [],
      "source": [
        "with open('story3_the_lucky_coin.txt') as f:\n",
        "    text_s3 = f.read()\n",
        "scores = sia.polarity_scores(text_s3)\n",
        "print(f\"Sentiment Scores: {scores}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ud2EMb_WVQWr"
      },
      "source": [
        "### Plotting Sentiment\n",
        "\n",
        "- Matplotlib https://matplotlib.org/\n",
        "\n",
        "This code snippet performs detailed sentiment analysis on a given text, specifically Story 3, 'The Lucky Coin', and visualizes the sentiment trends for each sentence.\n",
        "\n",
        "First, it uses the SentimentIntensityAnalyzer (sia) from the VADER module to evaluate sentiment. The text is cleaned by removing quotation marks and certain punctuation using regular expressions (re.sub). Next, the nltk.sent_tokenize() function splits the cleaned text into individual sentences. Each sentence is then analyzed for sentiment using the polarity_scores() method, which returns a dictionary containing four sentiment scores: positive, negative, neutral, and compound.\n",
        "\n",
        "The code proceeds by extracting each of these sentiment scores and organizing them into separate lists for positive, negative, neutral, and compound sentiment scores. It then prints the total number of sentences along with each sentence and its corresponding sentiment score for detailed inspection.\n",
        "\n",
        "Finally, the code generates a visual sentiment timeline using Matplotlib to plot the sentiment scores for each sentence. It creates a graph with four different sentiment curves: positive, negative, neutral, and compound. The graph provides a clear visual representation of how sentiment varies across the text, allowing for a more intuitive understanding of the emotional flow throughout the story. The plot includes labels, a title, a legend, and a grid to enhance readability and comprehension of the sentiment distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh9L3IMWVQWr"
      },
      "outputs": [],
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "clean_text = re.sub(r'[\"“”]', '', text_s3)\n",
        "\n",
        "sentences = nltk.sent_tokenize(clean_text) #split story into sentence\n",
        "\n",
        "# analyze sentiment for each sentence. the function polarity_scores() that takes a string of text and returns a dictionary of sentiment scores\n",
        "sentiment_scores = [sia.polarity_scores(sentence) for sentence in sentences]\n",
        "\n",
        "print(f\"Total sentences: {len(sentences)}\")\n",
        "\n",
        "# Extract individual sentiment scores: positive, negative, neutral, and compound\n",
        "positive_scores = [score['pos'] for score in sentiment_scores]\n",
        "negative_scores = [score['neg'] for score in sentiment_scores]\n",
        "neutral_scores = [score['neu'] for score in sentiment_scores]\n",
        "compound_scores = [score['compound'] for score in sentiment_scores]\n",
        "\n",
        "# sentence and its score\n",
        "for i, (sentence, score) in enumerate(zip(sentences, sentiment_scores), 1):\n",
        "    print(f\"{i}. {sentence} (Sentiment Score: {score})\")\n",
        "\n",
        "# Plot sentiment timeline\n",
        "plt.figure(figsize=(10, 5))\n",
        "# Plot positive, negative, and neutral sentiment scores\n",
        "plt.plot(range(1, len(sentences) + 1), positive_scores, marker='o', linestyle='-', color='green', label=\"Positive Sentiment\")\n",
        "plt.plot(range(1, len(sentences) + 1), negative_scores, marker='o', linestyle='-', color='red', label=\"Negative Sentiment\")\n",
        "plt.plot(range(1, len(sentences) + 1), neutral_scores, marker='o', linestyle='-', color='blue', label=\"Neutral Sentiment\")\n",
        "plt.plot(range(1, len(sentences) + 1), compound_scores, marker='o', linestyle='-', color='purple', label=\"Compound Sentiment\")\n",
        "\n",
        "\n",
        "# Labels and title\n",
        "plt.xlabel(\"Sentence Index\")\n",
        "plt.ylabel(\"Sentiment Score\")\n",
        "plt.title(\"Sentiment Curves for 'The Lucky Coin'\")\n",
        "plt.legend()\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nHLWMBnVQWr"
      },
      "outputs": [],
      "source": [
        "clean_text = re.sub(r'[\"“”]', '', text_s2)\n",
        "\n",
        "sentences = nltk.sent_tokenize(clean_text) #split story into sentence\n",
        "\n",
        "# analyze sentiment for each sentence. the function polarity_scores() that takes a string of text and returns a dictionary of sentiment scores\n",
        "sentiment_scores = [sia.polarity_scores(sentence)['compound'] for sentence in sentences]\n",
        "\n",
        "print(f\"Total sentences: {len(sentences)}\")\n",
        "print(f\"Total sentiment scores: {len(sentiment_scores)}\")\n",
        "print(f\"Last sentence detected: {sentences[-1]}\")\n",
        "\n",
        "# sentence and its score\n",
        "for i, (sentence, score) in enumerate(zip(sentences, sentiment_scores), 1):\n",
        "    print(f\"{i}. {sentence} (Sentiment Score: {score})\")\n",
        "\n",
        "# Plot sentiment timeline\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(sentences) + 1), sentiment_scores, marker='o', linestyle='-', color='b', label=\"Sentiment Score\")\n",
        "\n",
        "# Labels and title\n",
        "plt.axhline(y=0, color='red', linestyle='-', linewidth=0.9, label= \"Neutral Sentiment\")  # Neutral line\n",
        "plt.xlabel(\"Sentence Index\")\n",
        "plt.ylabel(\"Sentiment Score\")\n",
        "plt.title(\"Sentiment Timeline of 'The Forgotten Letter'\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G71Nycm_VQWr"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".senti",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}